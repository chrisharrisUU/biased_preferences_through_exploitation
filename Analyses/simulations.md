The following simulations were run *N* = 10000 times, simulating as many “participants”. Random draws were calculated for each simulation run individually. We refer to the two options here as left and right instead of A and B to increase readability.

### BIAS

To stay in line with the original paper (Fiedler, 1996), we describe memory instances as the columns to the memory matrix we create. Programmatically, we transposed this description such that each instance was represented by a row in our program as this made some operations easier to program.  
 We first created the perfect, noise-free binominal vectors of length *l* = 9 and with probability p = .5 for zeros and ones for one option (*left*) and one outcome (*win*). For both these vectors we then inverted *e* = 5 positions to create the second option (*right*) and outcome (*loss*). The two options and the two outcomes were thus, on average, slightly negatively correlated. The overall matrix consisted therefore of 100 columns (one per trial) and 18 rows (two vectors of length *l* = 9).  
Just like in the later experiments, we forced our BIAS model to sample a particular distribution of evidence, before then letting the model decide which option would be chosen on the next trial for the remainder of the simulation. Any given sampling trial would take place in the following order: Given the previous information (we will return to this momentarily) the model would make a choice for either the left or the right option. Then the outcome would be determined given the overall outcome probability for winning being either *p* = .75 (when we simulated a reward-rich environment) or *p* = .25 (when we simulated a reward-impoverished environment). The option chosen, and the outcome drawn would determine the information that would be saved in the column of the memory matrix on trial t. However, importantly, the model simulated noise by randomly inverting instances in the option as well as the outcome vectors. Specifically, a noise vector of the same length as all other vectors (*l* = 9) was created which was binominal and in which ones occurred at each instance with probability *noise* = .33. At the position of each one, the option vector was inversed. This process was repeated for the outcome vector. So, given a trial t with the chosen option left and the drawn outcome win, the memory representation would be *left’* and *win’*.  
This memory representation was correlated with the prompts (the ideal, noise-free vectoral representations) for either option to determine a weight. Then the memory representation was correlated with the prompts for either outcome to determine a match between memory traces and outcomes, from which we calculated an outcome match using the following formula: $\frac{r_{+}}{r_{+} + r_{-}}$. This outcome match was then weighted with the option match to determine the overall weighted match of this particular trial for either option. On each trial we updated a cumulative sum of these weighted matches per option so as to reduce the required computing resources. The option with the larger sum was chosen on the next trial (see above). For the initial evidence, we forced the model to select a particular option but updated the weighted match following the exact same procedure.  

### Bayesian learning model

The Bayesian learning model kept track of the total number of wins and losses with each option. On any given trial this information was used to calculate a beta-distribution for each option - the probability density functions for the two options, respectively. That is, due to the independence of the two options from one another, they were also modelled independently from one another. From these probability density functions three x values were sampled with probability y in order to compare the two options. In other words, as the probability density function describes the support in the data along the outcome probability continuum between 0 and 1, the function forms a maximum around the most likely outcome probability. We let our model sample three values with the most likely outcome probabilities being more likely to be sampled proportionate to the support in the data. From these samples, a mean was created which was regarded as the estimated probability for winning with this option. The option with the higher estimated probability was chosen, the outcome determined and saved in memory. Note, how for this model memory is perfect and the only noise lies in the sampling of estimated outcome probabilities.  

### Rescorla-Wagner  

The Rescorla-Wagner model, finally, integrates new information in an overall expected value estimation per option available. New information in integrated according to the learning rate, α, which simply weights new information before updating the overall expected value. However, the Rescorla-Wagner model does not always pick the option with the highest expected value, but does so depending on a soft-max function that relies on a second parameter β. As β approaches 0, the respective expected values are weighted less, and behavior becomes more exploratory until at β = 0, the probability of choosing any one option is at chance level. As β approaches ∞, the respective values are weighted more strongly, and behavior becomes more deterministic. The Rescorla-Wagner model was initiated with chance level starting values and the following functions for updating:  
$V_{s,t} = V_{s,t-1} + α(r_{t-1} - V_{s,t-1})$  
And for observation:  
$p(s) = \frac{exp(β * V_{s})}{\sum_{i} exp(β*V_{i})}$  